{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Global variables\n",
    "train_dir = 'svhn_large_data/train'\n",
    "test_dir = 'svhn_large_data/test'\n",
    "extra_dir = 'svhn_large_data/extra'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract image metadata\n",
    "def extract_metadata(dr):\n",
    "    metadata = {}\n",
    "    fl = h5py.File(dr + '/digitStruct.mat')\n",
    "    \n",
    "    for i in xrange(len([x for x in os.listdir(dr) if x.split('.')[1] == 'png'])):        \n",
    "        name = fl['digitStruct']['name'][i][0]\n",
    "        bbox = fl['digitStruct']['bbox'][i][0]\n",
    "        image_metadata = []\n",
    "    \n",
    "        if len(fl[bbox]['label']) == 1:    \n",
    "            digit_metadata = {}\n",
    "            digit_metadata['height'] = fl[bbox]['height'][0][0]\n",
    "            digit_metadata['width'] = fl[bbox]['width'][0][0]\n",
    "            digit_metadata['top'] = fl[bbox]['top'][0][0]\n",
    "            digit_metadata['left'] = fl[bbox]['left'][0][0]\n",
    "            digit_metadata['label'] = fl[bbox]['label'][0][0]\n",
    "            image_metadata.append(digit_metadata)                    \n",
    "        else:\n",
    "            for j in xrange(len(fl[bbox]['label'])):\n",
    "                digit_metadata = {}\n",
    "                digit_metadata['height'] = [x for x in fl[fl[bbox]['height'][j][0]]][0][0]\n",
    "                digit_metadata['width'] = [x for x in fl[fl[bbox]['width'][j][0]]][0][0]\n",
    "                digit_metadata['top'] = [x for x in fl[fl[bbox]['top'][j][0]]][0][0]\n",
    "                digit_metadata['left'] = [x for x in fl[fl[bbox]['left'][j][0]]][0][0]\n",
    "                digit_metadata['label'] = [x for x in fl[fl[bbox]['label'][j][0]]][0][0]\n",
    "                image_metadata.append(digit_metadata)                    \n",
    "        \n",
    "        metadata[''.join(chr(c) for c in fl[name][:])] = image_metadata\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "train_metadata = extract_metadata(train_dir)\n",
    "test_metadata = extract_metadata(test_dir)\n",
    "# extra_metadata = extract_metadata(extra_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 29930.png in the training set is the only image with more than 5 digits, so we will treat it as an outlier an remove it\n",
    "display.display(display.Image(train_dir + '/' + str(29930) + '.png'))\n",
    "if '29930.png' in train_metadata:\n",
    "    del train_metadata['29930.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at a few images\n",
    "for i in xrange(1, 3):\n",
    "    s = '\\n'\n",
    "    for j in xrange(len(train_metadata[str(i) + '.png'])):\n",
    "        s += str(i) + '.png digit ' + str(j) + ':'\n",
    "        s += ' height = ' + str(train_metadata[str(i) + '.png'][j]['height'])\n",
    "        s += ' wigth = ' + str(train_metadata[str(i) + '.png'][j]['width'])\n",
    "        s += ' top = ' + str(train_metadata[str(i) + '.png'][j]['top'])\n",
    "        s += ' left = ' + str(train_metadata[str(i) + '.png'][j]['left'])\n",
    "        s += ' label = ' + str(train_metadata[str(i) + '.png'][j]['label'])\n",
    "        s += '\\n' if j != len(train_metadata[str(i) + '.png']) - 1 else ''\n",
    "    print(s)\n",
    "    display.display(display.Image(train_dir + '/' + str(i) + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open images, convert to grayscale, resize, convert to numpy, and normalize\n",
    "# Create labels\n",
    "train_num = 25000\n",
    "img_size = 64\n",
    "channels_num = 1\n",
    "labels_num = 11\n",
    "digits_num = 5\n",
    "\n",
    "train_X = np.ndarray((train_num, img_size, img_size, channels_num)).astype(np.float32)\n",
    "train_y = np.ndarray((train_num, digits_num, labels_num))\n",
    "valid_X = np.ndarray((len(train_metadata) - train_num, img_size, img_size, channels_num)).astype(np.float32)\n",
    "valid_y = np.ndarray((len(train_metadata) - train_num, digits_num, labels_num))\n",
    "test_X = np.ndarray((len(test_metadata), img_size, img_size, channels_num)).astype(np.float32)\n",
    "test_y = np.ndarray((len(test_metadata), digits_num, labels_num))\n",
    "\n",
    "# Training\n",
    "for i in xrange(1, len(train_X) + 1):\n",
    "    img = Image.open(train_dir + '/' + str(i) + '.png')\n",
    "    img_gray = img.convert('L')\n",
    "    img_resized = img_gray.resize((img_size, img_size), Image.ANTIALIAS)\n",
    "    img_np = np.array(img_resized)\n",
    "    img_normal = (img_np - (255.0 / 2.0)) / 255.0\n",
    "    img_reshaped = img_normal.reshape((img_size, img_size, channels_num))\n",
    "    train_X[i - 1,:,:,:] = img_reshaped\n",
    "    \n",
    "    for j in xrange(digits_num):\n",
    "        if j < len(train_metadata[str(i) + '.png']):\n",
    "            label = train_metadata[str(i) + '.png'][j]['label']\n",
    "            train_y[i - 1,j,:] = np.arange(labels_num) == (label if label != 10 else 0)\n",
    "        else:\n",
    "            train_y[i - 1,j,:] = np.arange(labels_num) == 10\n",
    "    \n",
    "# Validation\n",
    "for i in xrange(len(valid_X) + 1):\n",
    "    if i != 4929:  # Account for outlier\n",
    "        img = Image.open(train_dir + '/' + str(i + len(train_X) + 1) + '.png')\n",
    "        img_gray = img.convert('L')\n",
    "        img_resized = img_gray.resize((img_size, img_size), Image.ANTIALIAS)\n",
    "        img_np = np.array(img_resized)\n",
    "        img_normal = (img_np - (255.0 / 2.0)) / 255.0\n",
    "        img_reshaped = img_normal.reshape((img_size, img_size, channels_num))\n",
    "        valid_X[i if i < 4929 else i - 1,:,:,:] = img_reshaped\n",
    "\n",
    "        for j in xrange(digits_num):\n",
    "            if j < len(train_metadata[str(i + len(train_X) + 1) + '.png']):\n",
    "                label = train_metadata[str(i + len(train_X) + 1) + '.png'][j]['label']\n",
    "                index = i if i < 4929 else i - 1\n",
    "                valid_y[index,j,:] = np.arange(labels_num) == (label if label != 10 else 0)\n",
    "            else:\n",
    "                valid_y[index,j,:] = np.arange(labels_num) == 10\n",
    "\n",
    "# Testing\n",
    "for i in xrange(1, len(test_X) + 1):\n",
    "    img = Image.open(test_dir + '/' + str(i) + '.png')\n",
    "    img_gray = img.convert('L')\n",
    "    img_resized = img_gray.resize((img_size, img_size), Image.ANTIALIAS)\n",
    "    img_np = np.array(img_resized)\n",
    "    img_normal = (img_np - (255.0 / 2.0)) / 255.0\n",
    "    img_reshaped = img_normal.reshape((img_size, img_size, channels_num))\n",
    "    test_X[i - 1,:,:,:] = img_reshaped\n",
    "    \n",
    "    for j in xrange(digits_num):\n",
    "        if j < len(test_metadata[str(i) + '.png']):\n",
    "            label = test_metadata[str(i) + '.png'][j]['label']\n",
    "            test_y[i - 1,j,:] = np.arange(labels_num) == (label if label != 10 else 0)\n",
    "        else:\n",
    "            test_y[i - 1,j,:] = np.arange(labels_num) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure we preprocessed correctly\n",
    "print('Training data shape: ' + str(train_X.shape))\n",
    "print('Validation data shape: ' + str(valid_X.shape))\n",
    "print('Testing data shape: ' + str(test_X.shape))\n",
    "\n",
    "for i in xrange(1, 3):\n",
    "    s = '\\n'\n",
    "    for j in xrange(len(train_metadata[str(i) + '.png'])):\n",
    "        s += str(i) + '.png digit ' + str(j) + ':'\n",
    "        s += ' height = ' + str(train_metadata[str(i) + '.png'][j]['height'])\n",
    "        s += ' wigth = ' + str(train_metadata[str(i) + '.png'][j]['width'])\n",
    "        s += ' top = ' + str(train_metadata[str(i) + '.png'][j]['top'])\n",
    "        s += ' left = ' + str(train_metadata[str(i) + '.png'][j]['left'])\n",
    "        s += ' label = ' + str(train_metadata[str(i) + '.png'][j]['label'])\n",
    "        s += '\\n' if j != len(train_metadata[str(i) + '.png']) - 1 else ''\n",
    "    print(s)\n",
    "    \n",
    "%matplotlib inline  \n",
    "_, (sp1, sp2) = plt.subplots(1, 2)\n",
    "sp1.imshow(train_X[0].reshape(img_size, img_size), cmap=plt.cm.Greys);\n",
    "sp2.imshow(train_X[1].reshape(img_size, img_size), cmap=plt.cm.Greys);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define our performance metric\n",
    "def accuracy(preds, labels):\n",
    "    return 100 * np.sum(np.argmax(preds, 2) == np.argmax(labels, 2)) / (preds.shape[0] * preds.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"\"\" LINEAR NEURAL NETWORK \"\"\"\n",
    "\n",
    "# # Variables\n",
    "# steps_num = 1001\n",
    "# subset = 100\n",
    "# flat_data_size = img_size * img_size * channels_num\n",
    "# train_accuracies = []\n",
    "# valid_accuracies = []\n",
    "# losses = []\n",
    "\n",
    "# # Graph\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "#     # Reshape data\n",
    "#     train_X_flat = train_X.reshape((-1, flat_data_size))\n",
    "    \n",
    "#     # Data\n",
    "#     train_X_tf = tf.placeholder(tf.float32, (subset, flat_data_size))\n",
    "#     train_y_tf = tf.placeholder(tf.float32, (subset, digits_num, labels_num))\n",
    "\n",
    "#     # Weights\n",
    "#     weights_1 = tf.Variable(tf.truncated_normal((img_size * img_size, labels_num), stddev=0.1))\n",
    "#     weights_2 = tf.Variable(tf.truncated_normal((img_size * img_size, labels_num), stddev=0.1))\n",
    "#     weights_3 = tf.Variable(tf.truncated_normal((img_size * img_size, labels_num), stddev=0.1))\n",
    "#     weights_4 = tf.Variable(tf.truncated_normal((img_size * img_size, labels_num), stddev=0.1))\n",
    "#     weights_5 = tf.Variable(tf.truncated_normal((img_size * img_size, labels_num), stddev=0.1))\n",
    "\n",
    "#     # Biases\n",
    "#     bias_1 = tf.Variable(tf.zeros(labels_num))\n",
    "#     bias_2 = tf.Variable(tf.zeros(labels_num))\n",
    "#     bias_3 = tf.Variable(tf.zeros(labels_num))\n",
    "#     bias_4 = tf.Variable(tf.zeros(labels_num))\n",
    "#     bias_5 = tf.Variable(tf.zeros(labels_num))\n",
    "\n",
    "#     # Model\n",
    "#     def model(data):\n",
    "#         logits_1 = tf.matmul(data, weights_1) + bias_1\n",
    "#         logits_2 = tf.matmul(data, weights_2) + bias_2\n",
    "#         logits_3 = tf.matmul(data, weights_3) + bias_3\n",
    "#         logits_4 = tf.matmul(data, weights_4) + bias_4\n",
    "#         logits_5 = tf.matmul(data, weights_5) + bias_5\n",
    "#         return (logits_1, logits_2, logits_3, logits_4, logits_5)\n",
    "\n",
    "#     # Logits\n",
    "#     (train_logits_1, train_logits_2, train_logits_3, train_logits_4, train_logits_5) = model(train_X_tf)\n",
    "    \n",
    "#     # Loss\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(train_logits_1, train_y_tf[:,0,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_logits_2, train_y_tf[:,1,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_logits_3, train_y_tf[:,2,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_logits_4, train_y_tf[:,3,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_logits_5, train_y_tf[:,4,:]))\n",
    "\n",
    "#     # Optimizer\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "#     # Predictions\n",
    "#     train_preds = tf.pack((tf.nn.softmax(train_logits_1), \n",
    "#                           tf.nn.softmax(train_logits_2), \n",
    "#                           tf.nn.softmax(train_logits_3), \n",
    "#                           tf.nn.softmax(train_logits_4), \n",
    "#                           tf.nn.softmax(train_logits_5)), 1)\n",
    "    \n",
    "# # Session\n",
    "# with tf.Session(graph=graph) as session:\n",
    "#     tf.initialize_all_variables().run()\n",
    "    \n",
    "#     # Train\n",
    "#     for i in xrange(steps_num):\n",
    "#         batch_X = train_X_flat[:subset,:]\n",
    "#         batch_y = train_y[:subset,:]\n",
    "#         feed_dict = {train_X_tf: batch_X, train_y_tf: batch_y}\n",
    "#         _, l, preds = session.run([optimizer, loss, train_preds], feed_dict)\n",
    "        \n",
    "#         # Report training accuracy        \n",
    "#         if i % 100 == 0:\n",
    "#             train_accuracy = accuracy(preds, batch_y)\n",
    "#             train_accuracies.append(train_accuracy)\n",
    "#             losses.append(l)\n",
    "#             print('Step %d: Loss = %.2f, Training accuracy = %.2f%%\\n' % (i, l, train_accuracy))\n",
    "    \n",
    "# # Plot training graph to access under/overfitting\n",
    "# l, = plt.plot(losses, label='Loss')\n",
    "# ta, = plt.plot(train_accuracies, label='Training Accuracy (%)')\n",
    "# plt.xlabel('Training Steps (hundreds)')\n",
    "# plt.ylim(0, 100)\n",
    "# plt.xlim(0, 10)\n",
    "# plt.title('Overfitting')\n",
    "# plt.legend(handles=[ta, l])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"\"\" LINEAR NEURAL NETWORK \"\"\"\n",
    "\n",
    "# # Variables\n",
    "# batch_size = 32\n",
    "# reg_constant = 0.001\n",
    "# steps_num = 1001\n",
    "# steps_num_small = 251\n",
    "# subset = 100\n",
    "# flat_data_size = img_size * img_size * channels_num\n",
    "# train_accuracies = []\n",
    "# valid_accuracies = []\n",
    "# losses = []\n",
    "\n",
    "# # Graph\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "#     # Data\n",
    "#     train_X_flat = train_X.reshape((-1, flat_data_size))\n",
    "#     valid_X_flat = valid_X.reshape((-1, flat_data_size))\n",
    "#     test_X_flat = test_X.reshape((-1, flat_data_size))\n",
    "#     train_X_tf = tf.placeholder(tf.float32, (batch_size, flat_data_size))\n",
    "#     train_y_tf = tf.placeholder(tf.float32, (batch_size, digits_num, labels_num))\n",
    "#     valid_X_tf = tf.constant(valid_X_flat)\n",
    "#     test_X_tf = tf.constant(test_X_flat)\n",
    "\n",
    "#     # Weights\n",
    "#     weights_1 = tf.Variable(tf.random_normal((img_size * img_size, labels_num), stddev=0.1))\n",
    "#     weights_2 = tf.Variable(tf.random_normal((img_size * img_size, labels_num), stddev=0.1))\n",
    "#     weights_3 = tf.Variable(tf.random_normal((img_size * img_size, labels_num), stddev=0.1))\n",
    "#     weights_4 = tf.Variable(tf.random_normal((img_size * img_size, labels_num), stddev=0.1))\n",
    "#     weights_5 = tf.Variable(tf.random_normal((img_size * img_size, labels_num), stddev=0.1))\n",
    "\n",
    "#     # Biases\n",
    "#     bias_1 = tf.Variable(tf.zeros(labels_num))\n",
    "#     bias_2 = tf.Variable(tf.zeros(labels_num))\n",
    "#     bias_3 = tf.Variable(tf.zeros(labels_num))\n",
    "#     bias_4 = tf.Variable(tf.zeros(labels_num))\n",
    "#     bias_5 = tf.Variable(tf.zeros(labels_num))\n",
    "\n",
    "#     # Model\n",
    "#     def model(data):\n",
    "#         logits_1 = tf.matmul(data, weights_1) + bias_1\n",
    "#         logits_2 = tf.matmul(data, weights_2) + bias_2\n",
    "#         logits_3 = tf.matmul(data, weights_3) + bias_3\n",
    "#         logits_4 = tf.matmul(data, weights_4) + bias_4\n",
    "#         logits_5 = tf.matmul(data, weights_5) + bias_5\n",
    "#         return (logits_1, logits_2, logits_3, logits_4, logits_5)\n",
    "\n",
    "#     # Logits\n",
    "#     (train_logits_1, train_logits_2, train_logits_3, train_logits_4, train_logits_5) = model(train_X_tf)\n",
    "#     (valid_logits_1, valid_logits_2, valid_logits_3, valid_logits_4, valid_logits_5) = model(valid_X_tf)\n",
    "#     (test_logits_1, test_logits_2, test_logits_3, test_logits_4, test_logits_5) = model(test_X_tf)\n",
    "    \n",
    "#     # Loss\n",
    "#     regularization = reg_constant * (tf.nn.l2_loss(weights_1) + \n",
    "#                                      tf.nn.l2_loss(weights_2) + \n",
    "#                                      tf.nn.l2_loss(weights_3) + \n",
    "#                                      tf.nn.l2_loss(weights_4) + \n",
    "#                                      tf.nn.l2_loss(weights_5))\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(train_logits_1, train_y_tf[:,0,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_logits_2, train_y_tf[:,1,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_logits_3, train_y_tf[:,2,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_logits_4, train_y_tf[:,3,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_logits_5, train_y_tf[:,4,:]) + \n",
    "#                           regularization)\n",
    "\n",
    "#     # Optimizer\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "#     # Predictions\n",
    "#     train_preds = tf.pack((tf.nn.softmax(train_logits_1), \n",
    "#                           tf.nn.softmax(train_logits_2), \n",
    "#                           tf.nn.softmax(train_logits_3), \n",
    "#                           tf.nn.softmax(train_logits_4), \n",
    "#                           tf.nn.softmax(train_logits_5)), 1)\n",
    "\n",
    "#     valid_preds = tf.pack((tf.nn.softmax(valid_logits_1), \n",
    "#                           tf.nn.softmax(valid_logits_2), \n",
    "#                           tf.nn.softmax(valid_logits_3), \n",
    "#                           tf.nn.softmax(valid_logits_4), \n",
    "#                           tf.nn.softmax(valid_logits_5)), 1)\n",
    "\n",
    "#     test_preds = tf.pack((tf.nn.softmax(test_logits_1), \n",
    "#                          tf.nn.softmax(test_logits_2), \n",
    "#                          tf.nn.softmax(test_logits_3), \n",
    "#                          tf.nn.softmax(test_logits_4), \n",
    "#                          tf.nn.softmax(test_logits_5)), 1)\n",
    "\n",
    "# # Session\n",
    "# with tf.Session(graph=graph) as session:\n",
    "#     tf.initialize_all_variables().run()\n",
    "    \n",
    "#     for i in xrange(steps_num):\n",
    "#         # Get batch data and labels\n",
    "#         batch_start = batch_size * i % (train_y.shape[0] - batch_size)\n",
    "#         batch_end = batch_start + batch_size\n",
    "#         batch_X = train_X_flat[batch_start:batch_end,:]\n",
    "#         batch_y = train_y[batch_start:batch_end,:]\n",
    "#         feed_dict = {train_X_tf: batch_X, train_y_tf: batch_y}\n",
    "#         _, l, preds = session.run([optimizer, loss, train_preds], feed_dict)\n",
    "        \n",
    "#         # Report validation accuracy\n",
    "#         if i % 100 == 0:\n",
    "#             train_accuracy = accuracy(preds, batch_y)\n",
    "#             valid_accuracy = accuracy(valid_preds.eval(), valid_y)\n",
    "#             train_accuracies.append(train_accuracy)\n",
    "#             valid_accuracies.append(valid_accuracy)\n",
    "#             losses.append(l)\n",
    "#             print('Step %d: Loss = %.2f, Training accuracy = %.2f%%, Validation accuracy = %.2f%%\\n' % \n",
    "#                   (i, l, train_accuracy, valid_accuracy))\n",
    "\n",
    "#     print('Test set accuracy = %.2f%%' % accuracy(test_preds.eval(), test_y))\n",
    "        \n",
    "# # Plot training graph to access performance\n",
    "# l, = plt.plot(losses, label='Loss')\n",
    "# ta, = plt.plot(train_accuracies, label='Training Accuracy (%)')\n",
    "# va, = plt.plot(valid_accuracies, label='Validation Accuracy (%)')\n",
    "# plt.xlabel('Training Steps (hundreds)')\n",
    "# plt.ylim(0, 100)\n",
    "# plt.xlim(0, 10)\n",
    "# plt.title('learning Curve For Linear NN')\n",
    "# plt.legend(handles=[ta, va, l])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"\"\" DEEP NEURAL NETWORK \"\"\"\n",
    "\n",
    "# # Hyperparameters\n",
    "# batch_size = 32\n",
    "# reg_constant = 0.001\n",
    "# dropout_rate = 0.5\n",
    "# steps_num = 1001\n",
    "# flat_data_size = img_size * img_size * channels_num\n",
    "# hidden_first_nodes = 1024\n",
    "# hidden_second_nodes = 128\n",
    "# train_accuracies = []\n",
    "# valid_accuracies = []\n",
    "# losses = []\n",
    "\n",
    "# # Graph\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "#     # Data\n",
    "#     train_X_flat = train_X.reshape((-1, flat_data_size))\n",
    "#     valid_X_flat = valid_X.reshape((-1, flat_data_size))\n",
    "#     test_X_flat = test_X.reshape((-1, flat_data_size))\n",
    "#     train_X_tf = tf.placeholder(tf.float32, (batch_size, flat_data_size))\n",
    "#     train_y_tf = tf.placeholder(tf.float32, (batch_size, digits_num, labels_num))\n",
    "#     valid_X_tf = tf.constant(valid_X_flat)\n",
    "#     test_X_tf = tf.constant(test_X_flat)\n",
    "\n",
    "#     # First hidden weights\n",
    "#     weights_hidden_first_1 = tf.Variable(tf.random_normal((flat_data_size, hidden_first_nodes), stddev=0.1))\n",
    "#     weights_hidden_first_2 = tf.Variable(tf.random_normal((flat_data_size, hidden_first_nodes), stddev=0.1))\n",
    "#     weights_hidden_first_3 = tf.Variable(tf.random_normal((flat_data_size, hidden_first_nodes), stddev=0.1))\n",
    "#     weights_hidden_first_4 = tf.Variable(tf.random_normal((flat_data_size, hidden_first_nodes), stddev=0.1))\n",
    "#     weights_hidden_first_5 = tf.Variable(tf.random_normal((flat_data_size, hidden_first_nodes), stddev=0.1))\n",
    "\n",
    "#     # First hidden biases\n",
    "#     biases_hidden_first_1 = tf.Variable(tf.zeros(hidden_first_nodes))\n",
    "#     biases_hidden_first_2 = tf.Variable(tf.zeros(hidden_first_nodes))\n",
    "#     biases_hidden_first_3 = tf.Variable(tf.zeros(hidden_first_nodes))\n",
    "#     biases_hidden_first_4 = tf.Variable(tf.zeros(hidden_first_nodes))\n",
    "#     biases_hidden_first_5 = tf.Variable(tf.zeros(hidden_first_nodes))\n",
    "    \n",
    "#     # Second hidden weights\n",
    "#     weights_hidden_second_1 = tf.Variable(tf.random_normal((hidden_first_nodes, hidden_second_nodes), stddev=0.1))\n",
    "#     weights_hidden_second_2 = tf.Variable(tf.random_normal((hidden_first_nodes, hidden_second_nodes), stddev=0.1))\n",
    "#     weights_hidden_second_3 = tf.Variable(tf.random_normal((hidden_first_nodes, hidden_second_nodes), stddev=0.1))\n",
    "#     weights_hidden_second_4 = tf.Variable(tf.random_normal((hidden_first_nodes, hidden_second_nodes), stddev=0.1))\n",
    "#     weights_hidden_second_5 = tf.Variable(tf.random_normal((hidden_first_nodes, hidden_second_nodes), stddev=0.1))\n",
    "\n",
    "#     # Second hidden biases\n",
    "#     biases_hidden_second_1 = tf.Variable(tf.zeros(hidden_second_nodes))\n",
    "#     biases_hidden_second_2 = tf.Variable(tf.zeros(hidden_second_nodes))\n",
    "#     biases_hidden_second_3 = tf.Variable(tf.zeros(hidden_second_nodes))\n",
    "#     biases_hidden_second_4 = tf.Variable(tf.zeros(hidden_second_nodes))\n",
    "#     biases_hidden_second_5 = tf.Variable(tf.zeros(hidden_second_nodes))\n",
    "    \n",
    "#     # Output weights\n",
    "#     weights_output_1 = tf.Variable(tf.random_normal((hidden_second_nodes, labels_num), stddev=0.1))\n",
    "#     weights_output_2 = tf.Variable(tf.random_normal((hidden_second_nodes, labels_num), stddev=0.1))\n",
    "#     weights_output_3 = tf.Variable(tf.random_normal((hidden_second_nodes, labels_num), stddev=0.1))\n",
    "#     weights_output_4 = tf.Variable(tf.random_normal((hidden_second_nodes, labels_num), stddev=0.1))\n",
    "#     weights_output_5 = tf.Variable(tf.random_normal((hidden_second_nodes, labels_num), stddev=0.1))\n",
    "\n",
    "#     # Output biases\n",
    "#     biases_output_1 = tf.Variable(tf.zeros(labels_num))\n",
    "#     biases_output_2 = tf.Variable(tf.zeros(labels_num))\n",
    "#     biases_output_3 = tf.Variable(tf.zeros(labels_num))\n",
    "#     biases_output_4 = tf.Variable(tf.zeros(labels_num))\n",
    "#     biases_output_5 = tf.Variable(tf.zeros(labels_num)) \n",
    "\n",
    "#     # Model\n",
    "#     def model(data):\n",
    "#         # Compute first hidden layer\n",
    "#         hidden_first_1 = tf.nn.relu(tf.matmul(data, weights_hidden_first_1) + biases_hidden_first_1)\n",
    "#         hidden_first_2 = tf.nn.relu(tf.matmul(data, weights_hidden_first_2) + biases_hidden_first_2)\n",
    "#         hidden_first_3 = tf.nn.relu(tf.matmul(data, weights_hidden_first_3) + biases_hidden_first_3)\n",
    "#         hidden_first_4 = tf.nn.relu(tf.matmul(data, weights_hidden_first_4) + biases_hidden_first_4)\n",
    "#         hidden_first_5 = tf.nn.relu(tf.matmul(data, weights_hidden_first_5) + biases_hidden_first_5)\n",
    "                            \n",
    "#         # Compute second hidden layer\n",
    "#         hidden_second_1 = tf.nn.relu(tf.matmul(hidden_first_1, weights_hidden_second_1) + biases_hidden_second_1)\n",
    "#         hidden_second_2 = tf.nn.relu(tf.matmul(hidden_first_2, weights_hidden_second_2) + biases_hidden_second_2)\n",
    "#         hidden_second_3 = tf.nn.relu(tf.matmul(hidden_first_3, weights_hidden_second_3) + biases_hidden_second_3)\n",
    "#         hidden_second_4 = tf.nn.relu(tf.matmul(hidden_first_4, weights_hidden_second_4) + biases_hidden_second_4)\n",
    "#         hidden_second_5 = tf.nn.relu(tf.matmul(hidden_first_5, weights_hidden_second_5) + biases_hidden_second_5)\n",
    "        \n",
    "#         # Dropout regularization\n",
    "#         hidden_second_1 = tf.nn.dropout(hidden_second_1, dropout_rate)\n",
    "#         hidden_second_2 = tf.nn.dropout(hidden_second_2, dropout_rate)\n",
    "#         hidden_second_3 = tf.nn.dropout(hidden_second_3, dropout_rate)\n",
    "#         hidden_second_4 = tf.nn.dropout(hidden_second_4, dropout_rate)\n",
    "#         hidden_second_5 = tf.nn.dropout(hidden_second_5, dropout_rate)\n",
    "        \n",
    "#         # Compute output layer\n",
    "#         output_1 = tf.matmul(hidden_second_1, weights_output_1) + biases_output_1\n",
    "#         output_2 = tf.matmul(hidden_second_2, weights_output_2) + biases_output_2\n",
    "#         output_3 = tf.matmul(hidden_second_3, weights_output_3) + biases_output_3\n",
    "#         output_4 = tf.matmul(hidden_second_4, weights_output_4) + biases_output_4\n",
    "#         output_5 = tf.matmul(hidden_second_5, weights_output_5) + biases_output_5\n",
    "    \n",
    "#         # Return outputs\n",
    "#         return (output_1, output_2, output_3, output_4, output_5)\n",
    "\n",
    "#     # Logits\n",
    "#     (train_output_1, train_output_2, train_output_3, train_output_4, train_output_5) = model(train_X_tf)\n",
    "#     (valid_output_1, valid_output_2, valid_output_3, valid_output_4, valid_output_5) = model(valid_X_tf)\n",
    "#     (test_output_1, test_output_2, test_output_3, test_output_4, test_output_5) = model(test_X_tf)\n",
    "    \n",
    "#     # L2 regularization\n",
    "#     regularization = reg_constant * (tf.nn.l2_loss(weights_hidden_first_1) + \n",
    "#                                      tf.nn.l2_loss(weights_hidden_first_2) + \n",
    "#                                      tf.nn.l2_loss(weights_hidden_first_3) + \n",
    "#                                      tf.nn.l2_loss(weights_hidden_first_4) + \n",
    "#                                      tf.nn.l2_loss(weights_hidden_first_5) + \n",
    "#                                      tf.nn.l2_loss(weights_hidden_second_1) + \n",
    "#                                      tf.nn.l2_loss(weights_hidden_second_2) + \n",
    "#                                      tf.nn.l2_loss(weights_hidden_second_3) + \n",
    "#                                      tf.nn.l2_loss(weights_hidden_second_4) + \n",
    "#                                      tf.nn.l2_loss(weights_hidden_second_5) + \n",
    "#                                      tf.nn.l2_loss(weights_output_1) + \n",
    "#                                      tf.nn.l2_loss(weights_output_2) + \n",
    "#                                      tf.nn.l2_loss(weights_output_3) + \n",
    "#                                      tf.nn.l2_loss(weights_output_4) + \n",
    "#                                      tf.nn.l2_loss(weights_output_5))\n",
    "    \n",
    "#     # Loss\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(train_output_1, train_y_tf[:,0,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_output_2, train_y_tf[:,1,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_output_3, train_y_tf[:,2,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_output_4, train_y_tf[:,3,:]) + \n",
    "#                           tf.nn.softmax_cross_entropy_with_logits(train_output_5, train_y_tf[:,4,:]) + \n",
    "#                           regularization)\n",
    "\n",
    "#     # Optimizer\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "#     # Predictions\n",
    "#     train_preds = tf.pack((tf.nn.softmax(train_output_1), \n",
    "#                           tf.nn.softmax(train_output_2), \n",
    "#                           tf.nn.softmax(train_output_3), \n",
    "#                           tf.nn.softmax(train_output_4), \n",
    "#                           tf.nn.softmax(train_output_5)), 1)\n",
    "\n",
    "#     valid_preds = tf.pack((tf.nn.softmax(valid_output_1), \n",
    "#                           tf.nn.softmax(valid_output_2), \n",
    "#                           tf.nn.softmax(valid_output_3), \n",
    "#                           tf.nn.softmax(valid_output_4), \n",
    "#                           tf.nn.softmax(valid_output_5)), 1)\n",
    "\n",
    "#     test_preds = tf.pack((tf.nn.softmax(test_output_1), \n",
    "#                          tf.nn.softmax(test_output_2), \n",
    "#                          tf.nn.softmax(test_output_3), \n",
    "#                          tf.nn.softmax(test_output_4), \n",
    "#                          tf.nn.softmax(test_output_5)), 1)\n",
    "\n",
    "# # Session\n",
    "# with tf.Session(graph=graph) as session:\n",
    "#     tf.initialize_all_variables().run()\n",
    "    \n",
    "#     # Train\n",
    "#     for i in xrange(steps_num):\n",
    "#         batch_start = batch_size * i % (train_y.shape[0] - batch_size)\n",
    "#         batch_end = batch_start + batch_size\n",
    "#         batch_X = train_X_flat[batch_start:batch_end,:]\n",
    "#         batch_y = train_y[batch_start:batch_end,:]\n",
    "#         feed_dict = {train_X_tf: batch_X, train_y_tf: batch_y}\n",
    "#         _, l, preds = session.run([optimizer, loss, train_preds], feed_dict)        \n",
    "        \n",
    "#         # Report validation accuracy\n",
    "#         if i % 100 == 0:\n",
    "#             train_accuracy = accuracy(preds, batch_y)\n",
    "#             valid_accuracy = accuracy(valid_preds.eval(), valid_y)\n",
    "#             train_accuracies.append(train_accuracy)\n",
    "#             valid_accuracies.append(valid_accuracy)\n",
    "#             losses.append(l)\n",
    "\n",
    "#             print('Step %d: Loss = %.2f, Training accuracy = %.2f%%, Validation accuracy = %.2f%%\\n' % \n",
    "#                   (i, l, train_accuracy, valid_accuracy))\n",
    "        \n",
    "#     print('Test set accuracy = %.2f%%' % accuracy(test_preds.eval(), test_y))\n",
    "        \n",
    "# # Plot training graph to access performance\n",
    "# l, = plt.plot(losses, label='Loss')\n",
    "# ta, = plt.plot(train_accuracies, label='Training Accuracy (%)')\n",
    "# va, = plt.plot(valid_accuracies, label='Validation Accuracy (%)')\n",
    "# plt.xlabel('Training Steps (Hundreds)')\n",
    "# plt.ylim(0, 100)\n",
    "# plt.xlim(0, 10)\n",
    "# plt.title('learning Curve For Deep NN')\n",
    "# plt.legend(handles=[ta, va, l])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" CONVOLUTIONAL NEURAL NETWORK \"\"\"\n",
    "\n",
    "# Hyperparameters\n",
    "steps_num = 1001\n",
    "batch_size = 8\n",
    "patch_size = 5\n",
    "conv_depth_1 = 4\n",
    "conv_depth_2 = 8\n",
    "full_nodes = 256\n",
    "reg_constant = 0.001\n",
    "dropout_rate = 0.5\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "losses = []\n",
    "conv_stride = 1\n",
    "pool_window_size = [1, 2, 2, 1]\n",
    "pool_stride = [1, 2, 2, 1]\n",
    "padding = 'SAME'\n",
    "\n",
    "# Helper functions\n",
    "def create_weight(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def create_bias(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "def create_conv_layer(data, weights):\n",
    "    return tf.nn.conv2d(data, weights, [1, conv_stride, conv_stride, 1], padding)\n",
    "\n",
    "def create_pool_layer(data):\n",
    "    return tf.nn.max_pool(data, pool_window_size, pool_stride, padding)\n",
    "\n",
    "def get_conv_shape(data_shape, weights_shape):\n",
    "    if padding == 'VALID':\n",
    "        new_height = int(np.ceil((data_shape[1] - weights_shape[0] + 1) / conv_stride))\n",
    "        new_width = int(np.ceil((data_shape[2] - weights_shape[1] + 1) / conv_stride))\n",
    "    else:\n",
    "        new_height = int(np.ceil(data_shape[1] / conv_stride))\n",
    "        new_width = int(np.ceil(data_shape[2] / conv_stride))\n",
    "    return (data_shape[0], new_height, new_width, weights_shape[3])\n",
    "\n",
    "def get_pool_shape(data_shape):\n",
    "    new_height = int(np.ceil(data_shape[1] / pool_stride[1]))\n",
    "    new_width = int(np.ceil(data_shape[2] / pool_stride[2]))\n",
    "    return (data_shape[0], new_height, new_width, data_shape[3])\n",
    "\n",
    "# Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Data (don't flatten as we did before)\n",
    "    train_X_tf = tf.placeholder(tf.float32, (batch_size, img_size, img_size, channels_num))\n",
    "    train_y_tf = tf.placeholder(tf.float32, (batch_size, digits_num, labels_num))\n",
    "    valid_X_tf = tf.constant(valid_X)\n",
    "    test_X_tf = tf.constant(test_X)\n",
    "\n",
    "    # First convolution layer weights (5x5x1x16)\n",
    "    weights_conv_first_1 = create_weight((patch_size, patch_size, channels_num, conv_depth_1))\n",
    "    weights_conv_first_2 = create_weight((patch_size, patch_size, channels_num, conv_depth_1))\n",
    "    weights_conv_first_3 = create_weight((patch_size, patch_size, channels_num, conv_depth_1))\n",
    "    weights_conv_first_4 = create_weight((patch_size, patch_size, channels_num, conv_depth_1))\n",
    "    weights_conv_first_5 = create_weight((patch_size, patch_size, channels_num, conv_depth_1))\n",
    "        \n",
    "    # First convolution layer biases (16)\n",
    "    biases_conv_first_1 = create_bias((conv_depth_1,))\n",
    "    biases_conv_first_2 = create_bias((conv_depth_1,))\n",
    "    biases_conv_first_3 = create_bias((conv_depth_1,))\n",
    "    biases_conv_first_4 = create_bias((conv_depth_1,))\n",
    "    biases_conv_first_5 = create_bias((conv_depth_1,))\n",
    "\n",
    "    # Second convolution layer weights (5x5x16x32)\n",
    "    weights_conv_second_1 = create_weight((patch_size, patch_size, conv_depth_1, conv_depth_2))\n",
    "    weights_conv_second_2 = create_weight((patch_size, patch_size, conv_depth_1, conv_depth_2))\n",
    "    weights_conv_second_3 = create_weight((patch_size, patch_size, conv_depth_1, conv_depth_2))\n",
    "    weights_conv_second_4 = create_weight((patch_size, patch_size, conv_depth_1, conv_depth_2))\n",
    "    weights_conv_second_5 = create_weight((patch_size, patch_size, conv_depth_1, conv_depth_2))\n",
    "\n",
    "    # Second convolution layer biases (32)\n",
    "    biases_conv_second_1 = create_bias((conv_depth_2,))\n",
    "    biases_conv_second_2 = create_bias((conv_depth_2,))\n",
    "    biases_conv_second_3 = create_bias((conv_depth_2,))\n",
    "    biases_conv_second_4 = create_bias((conv_depth_2,))\n",
    "    biases_conv_second_5 = create_bias((conv_depth_2,))\n",
    "\n",
    "    # Calculate final pooled layer shape\n",
    "    conv_first_shape = get_conv_shape(train_X_tf.get_shape().as_list(), weights_conv_first_1.get_shape().as_list())\n",
    "    pool_first_shape = get_pool_shape(conv_first_shape)\n",
    "    conv_second_shape = get_conv_shape(pool_first_shape, weights_conv_second_1.get_shape().as_list())\n",
    "    pool_second_shape = get_pool_shape(conv_second_shape)\n",
    "    final_pooled_flat = pool_second_shape[1] * pool_second_shape[2] * pool_second_shape[3]\n",
    "        \n",
    "    # Fully connected layer weights (8192x1024)    \n",
    "    weights_full_1 = create_weight((final_pooled_flat, full_nodes))\n",
    "    weights_full_2 = create_weight((final_pooled_flat, full_nodes))\n",
    "    weights_full_3 = create_weight((final_pooled_flat, full_nodes))\n",
    "    weights_full_4 = create_weight((final_pooled_flat, full_nodes))\n",
    "    weights_full_5 = create_weight((final_pooled_flat, full_nodes))\n",
    "\n",
    "    # Fully connected layer biases (1024)\n",
    "    biases_full_1 = create_bias((full_nodes,))\n",
    "    biases_full_2 = create_bias((full_nodes,))\n",
    "    biases_full_3 = create_bias((full_nodes,))\n",
    "    biases_full_4 = create_bias((full_nodes,))\n",
    "    biases_full_5 = create_bias((full_nodes,))\n",
    "    \n",
    "    # Output weights\n",
    "    weights_output_1 = create_weight((full_nodes, labels_num))\n",
    "    weights_output_2 = create_weight((full_nodes, labels_num))\n",
    "    weights_output_3 = create_weight((full_nodes, labels_num))\n",
    "    weights_output_4 = create_weight((full_nodes, labels_num))\n",
    "    weights_output_5 = create_weight((full_nodes, labels_num))\n",
    "\n",
    "    # Output biases\n",
    "    biases_output_1 = create_bias((labels_num,))\n",
    "    biases_output_2 = create_bias((labels_num,))\n",
    "    biases_output_3 = create_bias((labels_num,))\n",
    "    biases_output_4 = create_bias((labels_num,))\n",
    "    biases_output_5 = create_bias((labels_num,)) \n",
    "    \n",
    "    # Model\n",
    "    def model(data):\n",
    "        # First convolution layer (64x64x1 to 64x64x16)\n",
    "        conv_first_1 = tf.nn.relu(create_conv_layer(data, weights_conv_first_1) + biases_conv_first_1)\n",
    "        conv_first_2 = tf.nn.relu(create_conv_layer(data, weights_conv_first_2) + biases_conv_first_2)\n",
    "        conv_first_3 = tf.nn.relu(create_conv_layer(data, weights_conv_first_3) + biases_conv_first_3)\n",
    "        conv_first_4 = tf.nn.relu(create_conv_layer(data, weights_conv_first_4) + biases_conv_first_4)\n",
    "        conv_first_5 = tf.nn.relu(create_conv_layer(data, weights_conv_first_5) + biases_conv_first_5)\n",
    "\n",
    "        # First pooling layer\n",
    "        pool_first_1 = create_pool_layer(conv_first_1)\n",
    "        pool_first_2 = create_pool_layer(conv_first_2)\n",
    "        pool_first_3 = create_pool_layer(conv_first_3)\n",
    "        pool_first_4 = create_pool_layer(conv_first_4)\n",
    "        pool_first_5 = create_pool_layer(conv_first_5)\n",
    "        \n",
    "        # Second convolution layer\n",
    "        conv_second_1 = tf.nn.relu(create_conv_layer(pool_first_1, weights_conv_second_1) + biases_conv_second_1)\n",
    "        conv_second_2 = tf.nn.relu(create_conv_layer(pool_first_2, weights_conv_second_2) + biases_conv_second_2)\n",
    "        conv_second_3 = tf.nn.relu(create_conv_layer(pool_first_3, weights_conv_second_3) + biases_conv_second_3)\n",
    "        conv_second_4 = tf.nn.relu(create_conv_layer(pool_first_4, weights_conv_second_4) + biases_conv_second_4)\n",
    "        conv_second_5 = tf.nn.relu(create_conv_layer(pool_first_5, weights_conv_second_5) + biases_conv_second_5)\n",
    "\n",
    "        # Second pooling layer\n",
    "        pool_second_1 = create_pool_layer(conv_second_1)\n",
    "        pool_second_2 = create_pool_layer(conv_second_2)\n",
    "        pool_second_3 = create_pool_layer(conv_second_3)\n",
    "        pool_second_4 = create_pool_layer(conv_second_4)\n",
    "        pool_second_5 = create_pool_layer(conv_second_5)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        data_size = pool_second_1.get_shape().as_list()[0]\n",
    "        full_1 = tf.nn.relu(tf.matmul(tf.reshape(pool_second_1, (data_size, final_pooled_flat)), \n",
    "                                      weights_full_1) + biases_full_1)\n",
    "        full_2 = tf.nn.relu(tf.matmul(tf.reshape(pool_second_2, (data_size, final_pooled_flat)), \n",
    "                                      weights_full_2) + biases_full_2)\n",
    "        full_3 = tf.nn.relu(tf.matmul(tf.reshape(pool_second_3, (data_size, final_pooled_flat)), \n",
    "                                      weights_full_3) + biases_full_3)\n",
    "        full_4 = tf.nn.relu(tf.matmul(tf.reshape(pool_second_4, (data_size, final_pooled_flat)), \n",
    "                                      weights_full_4) + biases_full_4)\n",
    "        full_5 = tf.nn.relu(tf.matmul(tf.reshape(pool_second_5, (data_size, final_pooled_flat)), \n",
    "                                      weights_full_5) + biases_full_5)\n",
    "        \n",
    "        # Output layer\n",
    "        output_1 = tf.matmul(full_1, weights_output_1) + biases_output_1\n",
    "        output_2 = tf.matmul(full_2, weights_output_2) + biases_output_2\n",
    "        output_3 = tf.matmul(full_3, weights_output_3) + biases_output_3\n",
    "        output_4 = tf.matmul(full_4, weights_output_4) + biases_output_4\n",
    "        output_5 = tf.matmul(full_5, weights_output_5) + biases_output_5\n",
    "    \n",
    "        # Return outputs\n",
    "        return (output_1, output_2, output_3, output_4, output_5)\n",
    "\n",
    "    # Logits\n",
    "    (train_output_1, train_output_2, train_output_3, train_output_4, train_output_5) = model(train_X_tf)\n",
    "    (valid_output_1, valid_output_2, valid_output_3, valid_output_4, valid_output_5) = model(valid_X_tf)\n",
    "    (test_output_1, test_output_2, test_output_3, test_output_4, test_output_5) = model(test_X_tf)\n",
    "    \n",
    "    # L2 regularization\n",
    "    regularization = reg_constant * (tf.nn.l2_loss(weights_conv_first_1) + \n",
    "                                     tf.nn.l2_loss(weights_conv_first_2) + \n",
    "                                     tf.nn.l2_loss(weights_conv_first_3) + \n",
    "                                     tf.nn.l2_loss(weights_conv_first_4) + \n",
    "                                     tf.nn.l2_loss(weights_conv_first_5) + \n",
    "                                     tf.nn.l2_loss(weights_conv_second_1) + \n",
    "                                     tf.nn.l2_loss(weights_conv_second_2) + \n",
    "                                     tf.nn.l2_loss(weights_conv_second_3) + \n",
    "                                     tf.nn.l2_loss(weights_conv_second_4) + \n",
    "                                     tf.nn.l2_loss(weights_conv_second_5) + \n",
    "                                     tf.nn.l2_loss(weights_full_1) + \n",
    "                                     tf.nn.l2_loss(weights_full_2) + \n",
    "                                     tf.nn.l2_loss(weights_full_3) + \n",
    "                                     tf.nn.l2_loss(weights_full_4) + \n",
    "                                     tf.nn.l2_loss(weights_full_5) + \n",
    "                                     tf.nn.l2_loss(weights_output_1) + \n",
    "                                     tf.nn.l2_loss(weights_output_2) + \n",
    "                                     tf.nn.l2_loss(weights_output_3) + \n",
    "                                     tf.nn.l2_loss(weights_output_4) + \n",
    "                                     tf.nn.l2_loss(weights_output_5))\n",
    "    \n",
    "    # Loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(train_output_1, train_y_tf[:,0,:]) + \n",
    "                          tf.nn.softmax_cross_entropy_with_logits(train_output_2, train_y_tf[:,1,:]) + \n",
    "                          tf.nn.softmax_cross_entropy_with_logits(train_output_3, train_y_tf[:,2,:]) + \n",
    "                          tf.nn.softmax_cross_entropy_with_logits(train_output_4, train_y_tf[:,3,:]) + \n",
    "                          tf.nn.softmax_cross_entropy_with_logits(train_output_5, train_y_tf[:,4,:]) + \n",
    "                          regularization)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "    # Predictions\n",
    "    train_preds = tf.pack((tf.nn.softmax(train_output_1), \n",
    "                          tf.nn.softmax(train_output_2), \n",
    "                          tf.nn.softmax(train_output_3), \n",
    "                          tf.nn.softmax(train_output_4), \n",
    "                          tf.nn.softmax(train_output_5)), 1)\n",
    "\n",
    "    valid_preds = tf.pack((tf.nn.softmax(valid_output_1), \n",
    "                          tf.nn.softmax(valid_output_2), \n",
    "                          tf.nn.softmax(valid_output_3), \n",
    "                          tf.nn.softmax(valid_output_4), \n",
    "                          tf.nn.softmax(valid_output_5)), 1)\n",
    "\n",
    "    test_preds = tf.pack((tf.nn.softmax(test_output_1), \n",
    "                         tf.nn.softmax(test_output_2), \n",
    "                         tf.nn.softmax(test_output_3), \n",
    "                         tf.nn.softmax(test_output_4), \n",
    "                         tf.nn.softmax(test_output_5)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Session\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    # Train\n",
    "    for i in xrange(101):\n",
    "        batch_start = batch_size * i % (train_y.shape[0] - batch_size)\n",
    "        batch_end = batch_start + batch_size\n",
    "        batch_X = train_X[batch_start:batch_end,:]\n",
    "        batch_y = train_y[batch_start:batch_end,:]\n",
    "        feed_dict = {train_X_tf: batch_X, train_y_tf: batch_y}\n",
    "        _, l, preds = session.run([optimizer, loss, train_preds], feed_dict)        \n",
    "\n",
    "        # Report validation accuracy\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy(preds, batch_y)\n",
    "#             valid_accuracy = accuracy(valid_preds.eval(), valid_y)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "#             valid_accuracies.append(valid_accuracy)\n",
    "#             losses.append(l)\n",
    "\n",
    "#             print('Step %d: Loss = %.2f, Training accuracy = %.2f%%, Validation accuracy = %.2f%%\\n' % \n",
    "#                   (i, l, train_accuracy, valid_accuracy))\n",
    "\n",
    "            print('Step %d: Training accuracy = %.2f%%\\n' % (i, train_accuracy))\n",
    "\n",
    "\n",
    "#     print('Test set accuracy = %.2f%%' % accuracy(test_preds.eval(), test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
